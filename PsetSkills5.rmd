---
title: "skills_ps_5"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(forcats)
library(dplyr)
library(ggplot2)
library(naniar)
library(lubridate)
library(nycflights13)
library(stringr)
library(naniar)
library(DescTools)
library(tidyr)



```

1.1

Here is the plot without any fixing: 

```{r}

rincome

ggplot(gss_cat,aes(x=rincome))+
geom_bar()
```

The prolem is that the categorial variable is not ordered. there are also many categories and this is confusing.  There are also categories that relate to lack of answere. Some of this can be fixed by using fct_infreq():




One way to fix it is to sort the values based on the frquency of each category using the fct_infreq command.  We also remove the "No answer" and "Not Applicable"

```{r}

gss_cat %>% 
filter(!(str_detect(rincome, "No answer|Not applicable|Don't know|Refuse"))) %>%  # I use str_detect to remove "No answer", Don't know Refuse  and "Not applicable"
ggplot( aes(x = fct_infreq(rincome))) + 
  geom_bar(fill="red") + 
  coord_flip()+ # I use this command to flip the axis
  labs(y="Number of observations", x="Wage brackets") +
  ggtitle("Distribution of wages ")

```
1.2
we can also lump more observations (factors) together by using the fct_lum command. To make the plot look nicer we order it again by using the fct_infreq command.  Again we remove the columns we dont want to see such as "Refused", "Other","Not applicable"
  
```{r}
gss_cat %>%
  filter(!(str_detect(rincome, "Refused|Not applicable|Other"))) %>%  # I use str_detect to remove "No answer" and "Not applicable"
  mutate(lump = fct_lump(rincome, n = 7)) %>%
  ggplot(aes(x=fct_infreq(lump)))+
  geom_bar(fill="blue")+ 
  coord_flip()+
  labs(y="Number of observations", x="Wage brackets") +
  ggtitle("Distribution of wages ")


levels(rincome)
```
1.3.a


This is the distribution of tvhours without any fixing of the graph:
```{r}

gss_cat
ggplot(gss_cat, aes(x = (tvhours))) + 
  geom_bar(fill="green") + 
  coord_flip()+
  labs(y="Number of observations", x="Hours of tv watch") +
  ggtitle("Distribution of tv - hours watched ")
```


```{r}

```


To better understand the outliers I have produced a boxplot the tv hours wathed by eacu religous group

```{r}
gss_cat %>% 
  filter(!(str_detect(relig, "Don't know|No answer"))) %>%  # I use str_detect to remove unwanted columns
ggplot(mapping = aes(x=relig, y=tvhours))+
    geom_boxplot()+ 
    theme(axis.text.x=element_text(angle=-45))
```
We consider the number outliers that are above 18 hours to be suspect as normal people need to sleep.  it might make sens to remove these.






```{r}

```





This 
option 3: winsorize


option 1: drop
The problem is that this removes the data and it is possible that we want to us it later. 

```{r}
gss_cat
relig_summary <- gss_cat %>%
  group_by(relig) %>%
  filter(tvhours<=18) %>% 
  summarise(
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(relig_summary, aes(tvhours, fct_reorder(relig,tvhours))) + geom_point()

```
option 2: set missing

```{r}

relig_summary <- gss_cat %>%
  replace_with_na(replace=list(gss_cat$tvhours>=18))%>%
  group_by(relig) %>%
  summarise(
    age = mean(age, na.rm = TRUE),
    tvhours = mean(tvhours, na.rm = TRUE),
    n = n()
  )
ggplot(relig_summary, aes(tvhours, fct_reorder(relig,tvhours))) + geom_point(color="orange")+
    labs(x="Hours wathed per religous group", y="Religion") +
    ggtitle("Hours of tv watched grouped by religion ")+
    theme(plot.title=element_text(hjust = 0.5))

```
Option 3: 
Here we use the funvtion winsorize to prepare the tvhours data.  we create a new variable wins_new_tvhours
```{r}




relig_wins<-gss_cat %>% 
  mutate(new_wins_tvhours=Winsorize(tvhours,probs=c(0.05,0.95),na.rm=TRUE, type=7))


relig_wins2<-relig_wins %>%
  group_by(relig) %>%
  summarise(
    tvhours_means = mean(new_wins_tvhours, na.rm = TRUE),
    n = n()
  ) 


  ggplot(relig_wins2, aes(y=tvhours_means, x = fct_reorder(relig,tvhours_means))) + 
  geom_point(color = "red")+


  labs(x="Hours wathed per religous group", y="Religion") +
  ggtitle("Hours of tv watched grouped by religion ")+
  theme(plot.title=element_text(hjust = 0.5))





```


1.3.c





```{r}



relig_summary <- gss_cat %>%
  group_by(relig) %>%
  summarise(
    tvhours_mean = mean(tvhours, na.rm = TRUE),
    error = qt(0.975,df=length(tvhours)-1)*sd(tvhours,na.rm=TRUE)/sqrt(length(tvhours)),
    LOWER95=mean(tvhours,na.rm=TRUE)-error,
    HIGHER95 =mean(tvhours, na.rm=TRUE)+error,
    n = n()
  )

relig_summary

ggplot(relig_summary)+
  
      geom_linerange(mapping=aes(x=relig,ymin=LOWER95,ymax=HIGHER95), colour='black') +
      geom_point(mapping=aes(x=relig,y=tvhours_mean))+
      theme(axis.text.x=element_text(angle=-45))

```
2.1
```{r}
d1 <- "123-Apr-03"
d2 <- "06-Jun-2017"
d3 <- "12/29/14" # Dec 29, 2014
d4 <- "November 20, 1909"
d5 <- c("January 2 (2016)", "January 2 (2018)")


parse_date_time(d1, "%y-m-d") # cannot find a way to work it 
parse_date_time(d2, "%d%m%y")
parse_date_time(d3, "%m%d%y")
parse_date_time(d4, "%m%d%y")

parse_date_time(d5, "%m%d%y")
```
2.2 

Seventh day of every month in 2012
```{r}
x <- seq.Date(from=as.Date("2012-01-7"), to=as.Date("2012-12-31"), by="month")

weekdays((x))
```
fifth day of every month in 2020:

```{r}
weekdays(seq.Date(from=as.Date("2012-01-5"), to=as.Date("2012-12-31"), by="month"))
```


2.3.1
```{r}
flights %>% 
  mutate(dep_diff=dep_time-sched_dep_time-dep_delay) %>% 
  mutate(dep_date=ISOdate(year,month,day)) %>%
  ggplot(aes(x=dep_date,y=dep_diff, color = dep_diff)) +
  geom_point()+
  ylim(-100,100)
  


```
There is often a difference, Why I dont know.  The difference is -40, 40, 80. Because it is the same change it is robably related to the ca-hanges to summer time and winter time.  These are there for not delays but simply the moveing og the clock


2.3.2


  
  
  
```{r}

  spring=ISOdate(2013,3,20)
  summer=ISOdate(2013,6,21) 
  autumn=ISOdate(2013,9,22)
  winter=ISOdate(2013,12,21)
           

flights_season <- flights %>% 
  mutate(dep_date=ISOdate(year,month,day))%>%
  mutate(season = case_when(spring<=dep_date & dep_date<summer ~ "spring",
                            summer<=dep_date & dep_date<autumn ~ "summer",
                            autumn<=dep_date & dep_date<winter ~ "autumn",
                            dep_date > autumn | dep_date<spring ~ "winter"    
                            ))


flights_season  %>% 
  ggplot(aes(x=dep_time,color=season)) +
  geom_freqpoly()
```
  
  2.3.3

  
  
  
```{r}
  flights  %>% 
  ggplot(aes(x=arr_delay,color=weekdays(ISOdate(year,month,day)))) +
  geom_freqpoly()+
  xlim(-40,150)

```


```{r}
 


 flights  %>% 
  mutate(delay_length=case_when(arr_delay<60 ~"short",
                                  arr_delay>=60 ~"long"),na.rm=TRUE) %>% 
  ggplot(aes(x=arr_delay,color=weekdays(ISOdate(year,month,day)))) +
  geom_freqpoly()+
  facet_wrap(~delay_length, scales="free")
  
```
  3.1
  
```{r}
seconds <- as.numeric(Sys.time())

seconds

x = as.POSIXct(seconds,origin = "1970-01-01",tz = "UTC")

x



```
  
  3.2
  
```{r}
library(dslabs)

nlevels(movielens$genre)

```
  There are 901 genres
  
```{r}
levels(movielens$genres)

movielens %>% 
  filter(str_detect(genres,"Sci-Fi|Horror"))

```
  there are 20537 movies that are classified as sci-fi or horror
  
```{r}
 movielens  %>% 
  mutate(movietypes=case_when( str_detect(genres,"Sci-Fi|Horror")~"SFH",
                              str_detect(genres,"Sci-Fi|Horror")==FALSE ~"Not_SFH")) %>% 
  group_by(year,movietypes) %>% 
  summarise(n=n()) %>% 
  ggplot(mapping=aes(x=year,y=n,color =movietypes))+
  geom_point()
  

 movielens  %>% 
  mutate(movietypes=case_when( str_detect(genres,"Sci-Fi|Horror")~"SFH",
                              str_detect(genres,"Sci-Fi|Horror")==FALSE ~"Not_SFH")) %>% 
  group_by(year,movietypes)%>%
  summarise(n=n()) %>% 
   pivot_wider(names_from=movietypes,
               values_from = n) %>% 
   mutate(ratio=SFH/Not_SFH) %>% 
   arrange(desc(ratio))


```



1.3.3

```{r}
movie_ratings<- movielens %>% 
  mutate(day_of_rating =  weekdays(as.POSIXct(timestamp,origin = "1970-01-01",tz = "UTC"))) 

movie_ratings

ggplot(movie_ratings,aes(x=day_of_rating)) +
  geom_bar()



  
  
```

```{r}
movie_ratings2<- movielens %>% 
  mutate(day_of_rating =  weekdays(as.POSIXct(timestamp,origin = "1970-01-01",tz = "UTC"))) %>% 
   mutate(movietypes=case_when( str_detect(genres,"Sci-Fi|Horror")~"SFH",
                              str_detect(genres,"Sci-Fi|Horror")==FALSE ~"Not_SFH")) 

movie_ratings2

ggplot(movie_ratings2,aes(x=day_of_rating)) +
  geom_bar()+
  facet_wrap(~movietypes)+
        theme(axis.text.x=element_text(angle=-45))



```

  4.1
  
  
This is the code from problem 3 - I dont think it is needed: 
```{r}


colnames(flights_season)[14]<-"faa"

flights_season

flights

flights_delay <- flights_season %>% 
  left_join(airports,by="faa")

flights_delay


flights_season  %>% 
  ggplot(aes(x=dep_time,color=season)) +
  geom_freqpoly()


airports
```




```{r}
# I create flights1 to find the average arrive delay time for each destination
flights1<-flights %>% 
  group_by(dest) %>% 
  summarise(n=mean(arr_delay,na.rm=TRUE)) 

flights1


#I left join all flights with flights in newly created flights1 to get the average arrive delay time for each destination into flights2 
flights2<-flights %>% 
  left_join(flights1,key="dest")

flights2


flights2 %>%
  left_join(airports, c("dest"= "faa")) %>% 
  filter(lon>-140) %>%  # remove Hawaii and Alaska as the longditute is less than 140
  ggplot(aes(lon, lat)) +
    borders("state") +
    geom_point(aes(size=n, color=n)) + # Here I use the size of the average delay (n) to control the size of each point and color
    coord_quickmap()
    

```
  
  
  4.2 

  The lon lat and lon are in the airport database. We must therefore join lon and lat both with the destination and with the origin and destination.
  I first left joined the destination airports as I did in 4.1, changed the column names to reflect that they were destination.  Then I left joined the origin and changed the the column names.  Then I droppend the excessive columns 
  
  
```{r}

#left join the airports with the flight destination
flights3 <-flights %>%
  left_join(airports, c("dest"= "faa")) 
  
#see the resulting column names
colnames(flights3)

#rename columns
names(flights3)[21] <- "lat_dest"
names(flights3)[22] <- "lon_dest"

#left join the airports with the flight origination
flights4<- flights3 %>% 
    left_join(airports, c("origin"= "faa")) 

#see the resulting column names
colnames(flights4)

#rename columns
names(flights4)[28] <- "lat_orig"
names(flights4)[29] <- "lon_orig"

#drop excessive columns
flights4 <- flights4[-c(20,23:27,30:33)]

#see the resulting column names
colnames(flights4)

```
4.3

here I examin if planes(tailnum) are owned by one airline (carrier).  I start this work by using group and summarise

```{r}
flights5 <- flights %>% 
  group_by(tailnum,carrier) %>% 
  summarise() %>% # here I summarise all the pairs
  summarise(n=n()) %>% #here I summarise the number of carriers that have owned a particular tail
  filter(n>1) # her I arrange the tailnames in decreasing order
  
flights5
  
planes
```
There are 17 planes(tailnums) out of 4044 that have been owned by different carriers.  There are seven cases where the carrier name has been obmitted
  
  
  
4.4
plane_cohorts <- inner_join(flights,
  select(planes, tailnum, plane_year = year),
  by = "tailnum"
) %>%
  mutate(age = year - plane_year) %>%
  filter(!is.na(age)) %>%
  mutate(age = if_else(age > 25, 25L, age)) %>%
  group_by(age) %>%
  summarise(
    dep_delay_mean = mean(dep_delay, na.rm = TRUE),
    dep_delay_sd = sd(dep_delay, na.rm = TRUE),
    arr_delay_mean = mean(arr_delay, na.rm = TRUE),
    arr_delay_sd = sd(arr_delay, na.rm = TRUE),
    n_arr_delay = sum(!is.na(arr_delay)),
    n_dep_delay = sum(!is.na(dep_delay))
  )

```{r}
# Here I join the plan database with the plane dataabase, and get the year of build
flights6 <- inner_join(flights,
            select(planes, tailnum,build_year = year),
            by="tailnum") 

flights6
  

#Here I calculate the mean delays and the sd of delays.  I also calculate the number of no delays
flights7 <- flights6%>% 
  mutate(plane_age=year-build_year) %>% 
  mutate(plane_age=ifelse(plane_age>20,20,plane_age))%>% #grouping all planes older than 20 at 20
  group_by(plane_age) %>% 
  summarise(
        dep_delay_mean = mean(dep_delay, na.rm = TRUE),
    dep_delay_sd = sd(dep_delay, na.rm = TRUE),
    arr_delay_mean = mean(arr_delay, na.rm = TRUE),
    arr_delay_sd = sd(arr_delay, na.rm = TRUE),
    n_arr_delay = sum(!is.na(arr_delay)),
    n_dep_delay = sum(!is.na(dep_delay)))
    
    
  flights7
  
ggplot(flights7,aes(x=plane_age,y=dep_delay_mean))+
  geom_point()+
  scale_x_continuous("Age o plane") +
  scale_y_continuous("Mean departure delay")+
  ggtitle("Mean departure delays plotted against the age of planes ")

  

```

For the first 10 years the departure delay seems to increase with older age of planes.  After 10 years the average delay seems to fall again.  Delays of older planes do not seem to be deilayed a lot but they are probably flown a lot less.  I grouped all planes older than 20 at 20


4.4

Here I need to carefully join the weather and the flight database.  There is so much data that I must be careful not to start a code that never stops.  I therefore select the first 100 rows of flight using the head function

```{r}
start_time <- Sys.time()  # this is to set the starting time

flights8<- 
  flights %>%
  head(100) %>%  # limiting the number of rows to 100
  left_join(weather, by = "year")

end_time <- Sys.time() #this is the final time
end_time - start_time #this is the difference between starting time and finishing time

nrow(flights8)

nrow(flights)

```
Therefore this took 0.5 seconds to run.  The number of rows is 2.6 million

4.6 
left_join is a socallaed outer join, which means that every flight in the flights database is joined with hourly weather for the whole year (i.e. 24*365 weather reports).  The calculation is there for 100*24*365>1.6 million + inconsistencies, n.a.'s etc which add about 55% more rows on top.

therefore for the whole dataset we would get approximately
2.6 million times 
0.5  X 336776/100 =1684 seconds = 27 minutes
2.6 366776/100 = 9536 million rows 



4.7

```{r}
# Here I use my work from 4.1 to look at what happened in this one day

flights1<-flights %>% 
  filter(year==2013,month==6,day==13) %>% 
  group_by(dest) %>% 
  summarise(n=mean(arr_delay,na.rm=TRUE)) 

flights1


#I left join all flights with flights in newly created flights1 to get the average arrive delay time for each destination into flights2 
flights2<-flights %>% 
  left_join(flights1,key="dest")

flights2


flights2 %>%
  left_join(airports, c("dest"= "faa")) %>% 
  filter(lon>-140) %>%  # remove Hawaii and Alaska as the longditute is less than 140
  ggplot(aes(lon, lat)) +
    borders("state") +
    geom_point(aes(size=n, color=n)) + # Here I use the size of the average delay (n) to control the size of each point and color
    coord_quickmap()
```

4.8 


 
 
 
```{r}
anti_join(flights, airports, by = c("dest" = "faa")) 





```
 this command will drop all destinations in flights that have a match in airports.  It will therefore show all the flights that will land in one of the airports in airports.
 
 
```{r}
anti_join(airports, flights, by = c("faa"="dest")) 
```
 
 this commant will drop all airports that have matcj in flights.  It will therefore show all airports that are not to flown to by any flight in flights. These are mainly small regional airports.
 